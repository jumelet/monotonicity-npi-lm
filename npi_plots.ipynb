{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ENV_NAMES = [\n",
    "    \"ADV\",\n",
    "    \"COND\",\n",
    "    \"D-NEG\",\n",
    "    \"S-NEG\",\n",
    "    \"ONLY\",\n",
    "    \"QNT\",\n",
    "    \"QUES\",\n",
    "    \"SMP-Q\",\n",
    "    \"SUP\",\n",
    "]\n",
    "\n",
    "FULL_ENV_NAMES = [\n",
    "    \"adverbs\",\n",
    "    \"conditional\",\n",
    "    \"determiner_negation_biclausal\",\n",
    "    \"sentential_negation_biclausal\",\n",
    "    \"only\",\n",
    "    \"quantifier\",\n",
    "    \"questions\",\n",
    "    \"simplequestions\",\n",
    "    \"superlative\",\n",
    "]\n",
    "\n",
    "def mn2mt(mn):\n",
    "    \"\"\"model_name to model_type\"\"\"\n",
    "    if \"gulorgood\" in mn:\n",
    "        return \"full\"\n",
    "    if \"filter\" in mn:\n",
    "        return \"no_npi\"\n",
    "    return \"no_env\"\n",
    "\n",
    "\n",
    "with open('results/all_results.pickle', 'rb') as f:\n",
    "    raw_results = pickle.load(f)\n",
    "\n",
    "with open('results/probing_results_no_dup.pickle', 'rb') as f:\n",
    "    probing_results = pickle.load(f)\n",
    "    \n",
    "with open('results/median_rank_results.pickle', 'rb') as f:\n",
    "    median_rank_results = pickle.load(f)\n",
    "    \n",
    "\n",
    "for mn, result in raw_results.items():\n",
    "    if \"probe\" in probing_results[mn]:\n",
    "        raw_results[mn][\"probe\"] = probing_results[mn][\"probe\"]\n",
    "    if \"median_rank\" in median_rank_results[mn]:\n",
    "        raw_results[mn][\"median_rank\"] = median_rank_results[mn][\"median_rank\"]\n",
    "\n",
    "    \n",
    "clean_results = {\n",
    "    \"probe\": {\n",
    "        \"full\": [],\n",
    "        \"no_npi\": [],\n",
    "    },\n",
    "    \"warstadt\": {\n",
    "        \"full\": [],\n",
    "        \"no_env\": [[] for _ in FULL_ENV_NAMES],\n",
    "    },\n",
    "    \"median_rank\": {\n",
    "        \"full\": [],\n",
    "        \"no_env\": [[] for _ in FULL_ENV_NAMES],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, result in raw_results.items():\n",
    "    model_type = mn2mt(model_name)\n",
    "    \n",
    "    if model_type in [\"full\", \"no_npi\"]:\n",
    "        probe_result = [\n",
    "            result[\"probe\"][env][1,\"hx\"][\"accuracy\"] \n",
    "            for env in [\"uniform\"] + FULL_ENV_NAMES\n",
    "        ]\n",
    "\n",
    "        clean_results[\"probe\"][model_type].append(np.array(probe_result))\n",
    "    else:\n",
    "        env = [env for env in FULL_ENV_NAMES if env in model_name[:len(env)]][0]\n",
    "        env_idx = FULL_ENV_NAMES.index(env)\n",
    "\n",
    "        clean_results[\"median_rank\"][model_type][env_idx].append(result[\"median_rank\"][env][0])\n",
    "        clean_results[\"warstadt\"][model_type][env_idx].append(result[\"warstadt\"][\"warstadt\"][env])\n",
    "\n",
    "    if model_type == \"full\":\n",
    "        median_rank_result = [\n",
    "            result[\"median_rank\"][env][0]\n",
    "            for env in [\"all\"] + FULL_ENV_NAMES\n",
    "        ]\n",
    "        warstadt_result = [\n",
    "            result[\"warstadt\"][\"warstadt\"][env]\n",
    "            for env in FULL_ENV_NAMES\n",
    "        ]\n",
    "\n",
    "        clean_results[\"median_rank\"][model_type].append(np.array(median_rank_result))\n",
    "        clean_results[\"warstadt\"][model_type].append(np.array(warstadt_result))\n",
    "\n",
    "           \n",
    "\n",
    "for task, task_results in clean_results.items():\n",
    "    aggregator = np.median if task == \"median_rank\" else np.mean\n",
    "    \n",
    "    for model_type, result in task_results.items():\n",
    "        axis = 1 if model_type == \"no_env\" else 0\n",
    "        if task == \"median_rank\" and model_type == \"no_env\":\n",
    "            result.insert(0, [10, 10, 10])\n",
    "        clean_results[task][model_type] = aggregator(result, axis=axis), np.std(result, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as lines\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = 10, 7\n",
    "\n",
    "task2model_types = {\n",
    "    \"probe\": [\"Full (exp. 1)\", r\"Full $\\backslash$ NPI (exp. 4)\"], \n",
    "    \"median_rank\": [\"Full (exp. 3)\", r\"Full $\\backslash$ ENV$\\cap$NPI (exp. 5b)\"], \n",
    "    \"warstadt\": [\"Full (exp. 2)\", r\"Full $\\backslash$ ENV$\\cap$NPI (exp. 5a)\"], \n",
    "}\n",
    "\n",
    "task2title = {\n",
    "    \"probe\": \"Monotonicity probing accuracy\",\n",
    "    \"median_rank\": r\"LM decoder & monotonicity DC cosine similarity $-$ Median NPI rank\",\n",
    "    \"warstadt\": r\"NPI acceptability accuracy\"\n",
    "}\n",
    "\n",
    "task2caption = {\n",
    "    \"probe\": \"DC evaluated on held-out environment class\",\n",
    "    \"median_rank\": \"Environment class monotonicity DC trained on\",\n",
    "    \"warstadt\": \"Environment class\"\n",
    "}\n",
    "\n",
    "def plot_scores(results):\n",
    "    for task, result in results.items():\n",
    "        print(task)\n",
    "        fig = plt.figure()\n",
    "\n",
    "        means = np.array([arr[0] for arr in result.values()])\n",
    "        stds = np.array([arr[1] for arr in result.values()])\n",
    "\n",
    "        sizes = np.full(means.shape[1], 0.2)\n",
    "\n",
    "        fig, axs = plt.subplots(1,1)\n",
    "        axs.axis('off')\n",
    "\n",
    "        cvals = means\n",
    "        cmin, cmax = 0.25, 0.75\n",
    "\n",
    "        if task == \"median_rank\":\n",
    "            cvals = means - np.min(means) + 2\n",
    "            cvals = np.log(cvals) / np.max(np.log(cvals))\n",
    "            cvals = cvals / 2 + 0.5\n",
    "            cvals = cvals * -1 + 1.5 \n",
    "            cvals += (1 - np.max(cvals))\n",
    "\n",
    "        cvals = np.append(cvals, [np.zeros(means.shape[1])+.5], axis=0)\n",
    "        colors = plt.cm.PiYG(cvals)\n",
    "        \n",
    "        if task == \"median_rank\":\n",
    "            rounded_means = means.astype(np.int)\n",
    "            scores = rounded_means.astype(np.str)\n",
    "        elif stds is not None:\n",
    "            rounded_means = np.round(means, 2)\n",
    "            scores = rounded_means.astype(np.str)\n",
    "            rounded_stds = np.round(stds, 2)\n",
    "            str_stds = rounded_stds.astype(np.str)\n",
    "            for (i, j), mean in np.ndenumerate(scores):\n",
    "                std = str_stds[i, j]\n",
    "                mean = f\"{mean}0\" if len(mean) == 3 else mean\n",
    "                std = f\"{std}0\" if len(std) == 3 else std\n",
    "                scores[i, j] = f\"{mean}$_{{\\pm{std}}}$\"\n",
    "\n",
    "        env_names = [[\"All-ENV\"] + ENV_NAMES] if scores.shape[1] != len(ENV_NAMES) else [ENV_NAMES]\n",
    "        scores = np.append(scores, env_names, axis=0)\n",
    "\n",
    "        model_types = task2model_types[task]\n",
    "        \n",
    "        table = axs.table(\n",
    "            rowLabels=model_types+[\"\"],\n",
    "            rowLoc=\"right\",\n",
    "            cellText=scores,\n",
    "            cellColours=colors,\n",
    "            loc='center',\n",
    "            cellLoc='center',\n",
    "            colLoc=\"center\",\n",
    "            colWidths=sizes,\n",
    "            edges=\"closed\",\n",
    "        )\n",
    "        table.scale(1, 3)\n",
    "\n",
    "        for idx, cell in enumerate(table.properties()['child_artists']):\n",
    "            i, j = idx // cvals.shape[1], idx % cvals.shape[1]\n",
    "            cell.get_text().set_fontsize(18)\n",
    "\n",
    "            try:\n",
    "                cell_text = cell.get_text().get_text()\n",
    "                if idx < np.prod(cvals.shape) and not (cmin < cvals[i, j] < cmax):\n",
    "                    cell.get_text().set_color('white')\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        ## Table values layout\n",
    "        for row_cell in table.properties()['child_artists'][-scores.shape[0]:]:\n",
    "            row_cell.get_text().set_fontsize(18)\n",
    "            row_cell.set_text_props(fontweight=\"bold\")\n",
    "\n",
    "        ## Model name layout\n",
    "        for cell in table._cells:\n",
    "            if cell[0] == scores.shape[0]-1:\n",
    "                col_cell = table._cells[cell]\n",
    "                col_cell.set_color(\"w\")\n",
    "                col_cell.get_text().set_color(\"black\")\n",
    "                col_cell.set_text_props(fontweight=\"bold\")\n",
    "                col_cell.set_height(0.1)\n",
    "\n",
    "        ## Reset table cell borders\n",
    "        for key, cell in table.get_celld().items():\n",
    "            cell.set_linewidth(4)\n",
    "            cell.set_edgecolor(\"w\")\n",
    "\n",
    "        ## Title with adaptive padding\n",
    "        plt.title(task2title[task], pad=(-160)+(scores.shape[0]*20)-10, fontweight=\"bold\", fontsize=20)\n",
    "\n",
    "        ## Lower caption text\n",
    "        label_offset = 0.27 if scores.shape[0] == 4 else 0.3\n",
    "        plt.text(\n",
    "            0.5, \n",
    "            label_offset, \n",
    "            task2caption[task], \n",
    "            fontsize=18, \n",
    "            fontweight=\"normal\", \n",
    "            horizontalalignment='center'\n",
    "        )\n",
    "        \n",
    "        ## Uniform line divider\n",
    "        if \"All-ENV\" in env_names[0]:\n",
    "            bg_line = lines.Line2D(\n",
    "                [-.107,-.107], [0.475,.6], \n",
    "                linewidth=6,\n",
    "                color=\"white\",\n",
    "                transform=fig.transFigure, \n",
    "                figure=fig\n",
    "            )\n",
    "            line = lines.Line2D(\n",
    "                [-.107,-.107], [0.4,.62], \n",
    "                linewidth=2,\n",
    "                color=\"black\",\n",
    "                dashes=(2,1),\n",
    "                transform=fig.transFigure, \n",
    "                figure=fig\n",
    "            )\n",
    "            fig.lines.extend([bg_line, line])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        fig.savefig(f\"figures/{task}.pdf\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "plot_scores(clean_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
